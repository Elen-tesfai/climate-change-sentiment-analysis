{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56554293",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Random Forest Classifier\n",
    "\n",
    "This notebook demonstrates how to perform sentiment analysis on social media data using a **Random Forest Classifier**. The goal is to analyze public opinions about climate change by classifying social media posts as **positive**, **negative**, or **neutral**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a086dca",
   "metadata": {},
   "source": [
    "## Step 1: Import Necessary Libraries\n",
    "\n",
    "Make sure you import all the libraries needed for this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a4d64a-072b-46cd-95d1-4400343d88d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder  # Needed for encoding labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f38b9",
   "metadata": {},
   "source": [
    "## Step 2: Load the Dataset\n",
    "\n",
    "Now, load the dataset into a pandas DataFrame. Ensure that the path to the file is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae0db058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label  \\\n",
      "0      the effects of global warming are devastating  negative   \n",
      "1  we should promote renewable energy to save the...  positive   \n",
      "2  governments need to do more to combat climate ...  negative   \n",
      "3  there's no scientific consensus on climate change   neutral   \n",
      "4  the environment is deteriorating, and it is al...  negative   \n",
      "\n",
      "                                              tokens  \\\n",
      "0     ['effect', 'global', 'warming', 'devastating']   \n",
      "1  ['promote', 'renewable', 'energy', 'save', 'pl...   \n",
      "2  ['government', 'need', 'combat', 'climate', 'c...   \n",
      "3  [\"'s\", 'scientific', 'consensus', 'climate', '...   \n",
      "4  ['environment', 'deteriorating', ',', 'alarming']   \n",
      "\n",
      "                             cleaned_text  \n",
      "0       effect global warming devastating  \n",
      "1    promote renewable energy save planet  \n",
      "2   government need combat climate change  \n",
      "3  's scientific consensus climate change  \n",
      "4    environment deteriorating , alarming  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Correct absolute path\n",
    "file_path = r'C:\\Users\\su_te\\Documents\\climate-change-sentiment-analysis\\data\\cleaned_data.csv'\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f659fec",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Data\n",
    "\n",
    "In this step, we clean and preprocess the dataset to prepare it for machine learning. The goal is to ensure the data is in the right format for further analysis and modeling.\n",
    "\n",
    "### Tasks in this step:\n",
    "\n",
    "1. **Handle Missing Values**  \n",
    "   We check for any missing values in the dataset and handle them appropriately.\n",
    "\n",
    "2. **Label Encoding**  \n",
    "   Convert the textual labels (e.g., \"positive\", \"negative\", \"neutral\") into numerical values, as machine learning models work with numerical data.\n",
    "\n",
    "3. **Stopword Removal** (Optional)  \n",
    "   Remove common stopwords (like \"and\", \"the\", \"is\", etc.) from the tokens to reduce noise in the text data.\n",
    "\n",
    "4. **Tokenization & Lemmatization/Stemming** (Optional)  \n",
    "   Tokenization has already been applied, but we could perform lemmatization or stemming to reduce words to their base form (e.g., \"running\" becomes \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bc5f9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: text            0\n",
      "label           0\n",
      "tokens          0\n",
      "cleaned_text    0\n",
      "dtype: int64\n",
      "\n",
      "Encoded labels:\n",
      " label\n",
      "0    4\n",
      "2    2\n",
      "1    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Preprocessed data (first few rows):\n",
      "                                                 text  label  \\\n",
      "0      the effects of global warming are devastating      0   \n",
      "1  we should promote renewable energy to save the...      2   \n",
      "2  governments need to do more to combat climate ...      0   \n",
      "3  there's no scientific consensus on climate change      1   \n",
      "4  the environment is deteriorating, and it is al...      0   \n",
      "\n",
      "                                         tokens  \\\n",
      "0        [effect, global, warming, devastating]   \n",
      "1    [promote, renewable, energy, save, planet]   \n",
      "2   [government, need, combat, climate, change]   \n",
      "3  ['s, scientific, consensus, climate, change]   \n",
      "4     [environment, deteriorating, ,, alarming]   \n",
      "\n",
      "                             cleaned_text  \n",
      "0       effect global warming devastating  \n",
      "1    promote renewable energy save planet  \n",
      "2   government need combat climate change  \n",
      "3  's scientific consensus climate change  \n",
      "4    environment deteriorating , alarming  \n"
     ]
    }
   ],
   "source": [
    "# 1. Check for missing values in the dataset\n",
    "print(\"Missing values:\", data.isnull().sum())\n",
    "\n",
    "# 2. Label encoding for 'label' column (converting text labels to numerical values)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Encoding labels (positive -> 1, negative -> 0, neutral -> 2)\n",
    "data['label'] = encoder.fit_transform(data['label'])\n",
    "print(\"\\nEncoded labels:\\n\", data['label'].value_counts())\n",
    "\n",
    "# 3. Optional: Remove stopwords (if needed, or you can skip this step)\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Check if 'tokens' column contains string representations of lists\n",
    "# If so, convert them to actual lists using eval(), else directly apply stopwords removal\n",
    "\n",
    "# If 'tokens' is already a list, you can apply stopword removal directly.\n",
    "data['tokens'] = data['tokens'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Apply the stopword removal function to the 'tokens' column\n",
    "data['tokens'] = data['tokens'].apply(remove_stopwords)\n",
    "\n",
    "# 4. Optional: Lemmatization/Stemming (if needed, or you can skip this)\n",
    "# You can use nltk or spacy for this step, but it’s optional depending on your needs.\n",
    "\n",
    "# Checking the preprocessed data\n",
    "print(\"\\nPreprocessed data (first few rows):\\n\", data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e6968",
   "metadata": {},
   "source": [
    "## Step 4: Train/Test Split\n",
    "\n",
    "In this step, we split the dataset into training and testing subsets. The training data will be used to train the model, and the testing data will be used to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6794064d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 5\n",
      "Testing data size: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features (X) - cleaned text tokens converted to a form suitable for ML (vectorized)\n",
    "X = data['tokens'].apply(lambda x: ' '.join(x))  # Join tokens to form text again for vectorization\n",
    "y = data['label']  # Target labels (encoded)\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the size of the train and test data\n",
    "print(f\"Training data size: {X_train.shape[0]}\")\n",
    "print(f\"Testing data size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fb475",
   "metadata": {},
   "source": [
    "## Step 5: Vectorizing the Text Data\n",
    "\n",
    "Since we have text data (tokens), we'll need to convert it into a format suitable for machine learning. The most common technique is **TF-IDF** (Term Frequency-Inverse Document Frequency) vectorization, which transforms the text into numerical features that can be fed into machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bf12ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (samples x features): (5, 18)\n",
      "Testing data shape (samples x features): (2, 18)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Vectorizing the Text Data using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit the number of features (words) to 5000\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Print the shape of the transformed data (number of samples x number of features)\n",
    "print(f\"Training data shape (samples x features): {X_train_tfidf.shape}\")\n",
    "print(f\"Testing data shape (samples x features): {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181cd8b8",
   "metadata": {},
   "source": [
    "## Step 6: Make Predictions and Evaluate the Model\n",
    "\n",
    "After training the model, we will test it on the test data to evaluate its performance. This involves:\n",
    "\n",
    "1. **Training the Model**  \n",
    "   We will use a machine learning algorithm (e.g., Logistic Regression, Support Vector Machine, etc.) to train the model on the training data. The model learns the relationships between the features and the labels in the training set.\n",
    "\n",
    "2. **Making Predictions**  \n",
    "   After training, we will use the model to predict the labels (sentiment: positive, negative, or neutral) for the unseen test data. This allows us to evaluate how well the model generalizes to new data.\n",
    "\n",
    "3. **Evaluating the Model's Performance**  \n",
    "   To assess the performance of the model, we will use several evaluation metrics:\n",
    "   - **Accuracy**: This measures the proportion of correct predictions (both true positives and true negatives) out of all predictions made.\n",
    "   - **Precision**: This tells us the proportion of correct positive predictions out of all predicted positives. It helps to understand how many of the predicted positive labels were actually correct.\n",
    "   - **Recall**: This measures the proportion of correct positive predictions out of all actual positives. It helps us understand how well the model detects all true positive instances.\n",
    "   - **F1-Score**: This is the harmonic mean of precision and recall. It's useful when there’s an imbalance between classes (e.g., if there are far more positive labels than negative ones).\n",
    "\n",
    "By using these metrics, we can determine the strengths and weaknesses of our model and decide whether further improvements or model tuning are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "636df5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n",
      "Precision: 0.75\n",
      "Recall: 0.50\n",
      "F1-Score: 0.33\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. Train the model (e.g., using Logistic Regression)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 2. Make predictions on the test data\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# 3. Evaluate the model's performance\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "# F1-Score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d6e06",
   "metadata": {},
   "source": [
    "### Result Breakdown:\n",
    "\n",
    "**Accuracy: 50.00%**\n",
    "\n",
    "This indicates that the model correctly predicted the sentiment for 50% of the test samples.\n",
    "\n",
    "While not ideal, this is not completely unexpected given that your dataset might be small (only 7 samples, which is a very small test size).\n",
    "\n",
    "---\n",
    "\n",
    "**Precision: 0.75**\n",
    "\n",
    "This means that when the model predicted a sentiment (positive or negative), it was correct 75% of the time.\n",
    "\n",
    "A relatively high precision shows that, when the model makes a positive prediction, it is more likely to be correct.\n",
    "\n",
    "---\n",
    "\n",
    "**Recall: 0.50**\n",
    "\n",
    "This means that the model correctly identified 50% of all the actual positive labels.\n",
    "\n",
    "Lower recall means that the model missed some positive samples.\n",
    "\n",
    "---\n",
    "\n",
    "**F1-Score: 0.33**\n",
    "\n",
    "The F1-Score is low because it's the harmonic mean of precision and recall. It tends to be lower when there is an imbalance between precision and recall, like in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e51845",
   "metadata": {},
   "source": [
    "## Step 7: Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac92f9b6",
   "metadata": {},
   "source": [
    "1. **Hyperparameter Tuning with GridSearchCV**  \n",
    "   This is the process of finding the best parameters for your model to optimize its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99cc679e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'alpha': 0.5}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           2       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.75      0.50      0.33         2\n",
      "weighted avg       0.75      0.50      0.33         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the parameter grid for Naive Bayes\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1, 2, 5]  # Smoothing parameter for Naive Bayes\n",
    "}\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# Initialize Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Initialize GridSearchCV with Leave-One-Out Cross-Validation\n",
    "grid_search = GridSearchCV(nb_model, param_grid, cv=loo, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search on training data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on test data\n",
    "y_pred_best = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred_best, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb734d",
   "metadata": {},
   "source": [
    "2. Try Different Models (e.g., Random Forest)\n",
    "\n",
    "Testing different models might give better results. We will try a **Random Forest Classifier** to compare its performance with the **Naive Bayes** model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb23a22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 50.00%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           2       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.75      0.50      0.33         2\n",
      "weighted avg       0.75      0.50      0.33         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf * 100:.2f}%\")\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred_rf, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c45a7b0",
   "metadata": {},
   "source": [
    "### **Random Forest Results**:\n",
    "\n",
    "- **Accuracy**: 50%  \n",
    "  The model correctly predicted sentiment for half of the test samples.\n",
    "\n",
    "- **Precision**: 0.75  \n",
    "  When the model predicted a sentiment (positive or negative), it was correct 75% of the time.\n",
    "\n",
    "- **Recall**: 0.50  \n",
    "  The model correctly identified 50% of all actual positive samples.\n",
    "\n",
    "- **F1-Score**: 0.33  \n",
    "  The balance between precision and recall is low, suggesting room for improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### **Issues to Address**:\n",
    "\n",
    "1. **Small Dataset**:  \n",
    "   With only 2 samples in the test set, these results aren't fully indicative of the model's true performance. A larger dataset would give more meaningful insights.\n",
    "\n",
    "2. **Class Imbalance**:  \n",
    "   If certain classes (e.g., positive, negative, neutral) are underrepresented in the dataset, it could lead to skewed precision and recall metrics.\n",
    "\n",
    "3. **Overfitting**:  \n",
    "   Given the small sample size, the models might be overfitting to the training data. This could explain the relatively high precision for some classes and low recall for others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed830c",
   "metadata": {},
   "source": [
    "## 3. Cross-Validation for Better Model Evaluation\n",
    "\n",
    "Cross-validation helps evaluate your model's performance more reliably by splitting the data into multiple subsets (folds) and testing the model on each fold. This reduces the risk of bias that can arise from using a single train-test split.\n",
    "\n",
    "Here’s how you can implement K-fold cross-validation for a Random Forest model:\n",
    "\n",
    "#### Using Cross-Validation with Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4d5b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: {np.int64(0): np.int64(3), np.int64(1): np.int64(1), np.int64(2): np.int64(1)}\n",
      "Random Forest Cross-Validation F1 Scores (Filtered): [1. 1.]\n",
      "Mean Random Forest F1 Score (Filtered): 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Check the class distribution to identify classes with too few samples\n",
    "class_counts = np.bincount(y_train)\n",
    "print(f\"Class distribution: {dict(zip(np.unique(y_train), class_counts))}\")\n",
    "\n",
    "# Filter out the class with only one sample (or classes with few samples)\n",
    "min_class_size = 2  # Define the threshold for acceptable class size\n",
    "valid_classes = np.where(class_counts >= min_class_size)[0]\n",
    "filtered_indices = np.where(np.isin(y_train, valid_classes))[0]\n",
    "\n",
    "# Filter the data\n",
    "X_train_filtered = X_train_tfidf[filtered_indices]\n",
    "y_train_filtered = y_train.iloc[filtered_indices]\n",
    "\n",
    "# Check if there are enough samples for at least 2 splits\n",
    "min_class_samples = min(class_counts[valid_classes])  # Find the smallest class size after filtering\n",
    "if min_class_samples >= 2:\n",
    "    # Use StratifiedKFold to preserve the class distribution in each fold\n",
    "    stratified_kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n",
    "    # Re-run cross-validation with the filtered data\n",
    "    cv_scores = cross_val_score(rf_model, X_train_filtered, y_train_filtered, cv=stratified_kfold, scoring='f1_macro')\n",
    "    # Output the cross-validation scores\n",
    "    print(f\"Random Forest Cross-Validation F1 Scores (Filtered): {cv_scores}\")\n",
    "    print(f\"Mean Random Forest F1 Score (Filtered): {cv_scores.mean()}\")\n",
    "else:\n",
    "    print(\"Not enough samples for valid cross-validation. Try reducing the class size threshold or adjusting the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23485e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
